# âš¡ Transformer Explained

> A free, interactive course explaining how Transformer models and Large Language Models work. No math degree required.

**GitHub Pages:** [https://kessousid.github.io/transformer-explained/](https://kessousid.github.io/transformer-explained/)

**Streamlit App:** [https://transformer-explained.streamlit.app](https://transformer-explained.streamlit.app) *(deploy via Streamlit Cloud â€” see below)*

---

## ğŸ“š Course Overview

11 interactive lessons across 4 modules â€” from zero to understanding modern LLMs.

### Module 1: Foundations
- Lesson 1: Introduction to Transformers
- Lesson 2: Tokenization
- Lesson 3: Word Embeddings

### Module 2: The Attention Mechanism
- Lesson 4: The Sequence Problem (Why RNNs failed)
- Lesson 5: The Attention Mechanism
- Lesson 6: Self-Attention
- Lesson 7: Multi-Head Attention

### Module 3: The Architecture
- Lesson 8: Positional Encoding
- Lesson 9: The Encoder
- Lesson 10: The Decoder

### Module 4: Modern LLMs
- Lesson 11: The Full Transformer & Modern LLMs

---

## âœ¨ Features

- ğŸ¯ **Beginner-friendly** â€” No prior ML knowledge required
- ğŸ¨ **Beautiful dark UI** â€” Professional AI-themed design
- âš¡ **Interactive visualizations** â€” Canvas-based diagrams for every concept
- ğŸ§  **Animated slideshows** â€” 5â€“7 slides per lesson with smooth animations
- ğŸ“Š **Live demos** â€” Tokenize your own text, explore embedding spaces, visualize attention
- âœ… **Quizzes** â€” Knowledge checks after every lesson
- ğŸ“ˆ **Progress tracking** â€” Your progress is saved locally
- ğŸ“± **Responsive** â€” Works on mobile and desktop

---

## ğŸš€ Getting Started

### View locally (static site)
Just open `index.html` in your browser â€” no server required!

### Run locally (Streamlit)
```bash
pip install streamlit
streamlit run streamlit_app.py
```

### Deploy to Streamlit Cloud (free)
1. Go to **[share.streamlit.io](https://share.streamlit.io)**
2. Sign in with GitHub
3. Click **New app** â†’ select `kessousid/transformer-explained`
4. Main file: `streamlit_app.py`
5. Click **Deploy** â†’ live in ~60 seconds!

### GitHub Pages (already live)
**[https://kessousid.github.io/transformer-explained/](https://kessousid.github.io/transformer-explained/)**

---

## ğŸ—‚ï¸ Project Structure

```
/
â”œâ”€â”€ index.html              # Landing page
â”œâ”€â”€ README.md
â”œâ”€â”€ css/
â”‚   â””â”€â”€ styles.css          # Complete design system (~600 lines)
â”œâ”€â”€ js/
â”‚   â”œâ”€â”€ app.js              # Navigation, slideshow, quiz, progress
â”‚   â””â”€â”€ visualizations.js   # Canvas-based interactive diagrams
â””â”€â”€ lessons/
    â”œâ”€â”€ 01-introduction.html
    â”œâ”€â”€ 02-tokenization.html
    â”œâ”€â”€ 03-embeddings.html
    â”œâ”€â”€ 04-the-problem.html
    â”œâ”€â”€ 05-attention.html
    â”œâ”€â”€ 06-self-attention.html
    â”œâ”€â”€ 07-multihead-attention.html
    â”œâ”€â”€ 08-positional-encoding.html
    â”œâ”€â”€ 09-encoder.html
    â”œâ”€â”€ 10-decoder.html
    â””â”€â”€ 11-full-transformer.html
```

---

## ğŸ§  Concepts Covered

| Concept | Description |
|---|---|
| Tokenization | How text is split into tokens (BPE) |
| Embeddings | Dense vector representations of tokens |
| Attention | Q, K, V mechanism â€” the core innovation |
| Self-Attention | How sequences attend to themselves |
| Multi-Head Attention | Parallel attention heads for richer understanding |
| Positional Encoding | Sinusoidal encoding of word order |
| Encoder | Reading and understanding input |
| Decoder | Auto-regressive text generation |
| LLM Variants | BERT (encoder-only), GPT (decoder-only), T5 (enc-dec) |

---

## ğŸ”¬ Based On

- **"Attention Is All You Need"** â€” Vaswani et al., 2017 ([arxiv](https://arxiv.org/abs/1706.03762))
- The original GPT, BERT, and T5 papers
- Anthropic's and OpenAI's public research

---

## ğŸ“„ License

MIT â€” Free to use, share, and modify.

---

*Built for AI learners everywhere. If this helped you, â­ star the repo!*
